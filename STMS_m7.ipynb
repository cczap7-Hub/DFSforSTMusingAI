{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d4372-9b54-4037-a594-57cd66a9ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sgp4.api import Satrec\n",
    "from sgp4.conveniences import jday\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, StringVar, OptionMenu \n",
    "import os\n",
    "import random\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import json \n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "# ================================\n",
    "# Read TLE data from file\n",
    "# ================================\n",
    "def read_tle_from_txt(filename, num_samples):\n",
    "    try:\n",
    "        with open(filename, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        tle_data = []\n",
    "        for i in range(len(lines)):\n",
    "            if \"1 \" in lines[i] and i + 1 < len(lines) and \"2 \" in lines[i + 1]:\n",
    "                tle_data.append((lines[i - 1].strip(), lines[i].strip(), lines[i + 1].strip()))\n",
    "        \n",
    "        if len(tle_data) > num_samples:\n",
    "            tle_data = random.sample(tle_data, num_samples)\n",
    "        \n",
    "        return tle_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Benchmark file {filename} not found.\")\n",
    "        return None\n",
    "\n",
    "def read_simulated_orbit_file(filename, num_points=10):\n",
    "    positions = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "            _, x, y, z = parts\n",
    "            positions.append([float(x), float(y), float(z)])\n",
    "\n",
    "    data = np.array(positions)\n",
    "    sequences = []\n",
    "\n",
    "    for i in range(len(data) - num_points):\n",
    "        pos_seq = data[i:i + num_points]\n",
    "        vel_seq = np.gradient(pos_seq, axis=0)  # Estimate velocity\n",
    "        seq = np.concatenate([pos_seq, vel_seq], axis=1)  # shape (10, 6)\n",
    "        sequences.append(seq)\n",
    "\n",
    "    return np.array(sequences)  # shape: (N, 10, 6)\n",
    "\n",
    "# ================================\n",
    "# Build LSTM Model\n",
    "# ================================\n",
    "def build_lstm_model():\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=(19, 6)),\n",
    "        layers.Masking(mask_value=0.0),\n",
    "        layers.Bidirectional(layers.LSTM(\n",
    "            256, return_sequences=True, \n",
    "            dropout=0.2, recurrent_dropout=0.2\n",
    "        )),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Bidirectional(layers.LSTM(\n",
    "            256, \n",
    "            dropout=0.2, recurrent_dropout=0.2\n",
    "        )),  # <-- remove return_sequences=True here!\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(3)\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_gru_model():\n",
    "    model = keras.Sequential([\n",
    "        Input(shape=(19, 6)),\n",
    "        layers.Masking(mask_value=0.0),\n",
    "        layers.Bidirectional(layers.GRU(\n",
    "            128, return_sequences=True, \n",
    "            dropout=0.2, recurrent_dropout=0.2\n",
    "        )),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Bidirectional(layers.GRU(\n",
    "            128, \n",
    "            dropout=0.2, recurrent_dropout=0.2\n",
    "        )),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(3)\n",
    "    ])\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# ================================\n",
    "# Get satellite position and velocity\n",
    "# ================================\n",
    "def get_position_velocity(satellite, date_time):\n",
    "    jd, fr = jday(date_time.year, date_time.month, date_time.day,\n",
    "                  date_time.hour, date_time.minute, date_time.second + date_time.microsecond * 1e-6)\n",
    "    e, r, v = satellite.sgp4(jd, fr)\n",
    "    return np.array(r, dtype=np.float32) if e == 0 else None\n",
    "\n",
    "# ================================\n",
    "# Generate Benchmark Positions\n",
    "# ================================\n",
    "def generate_benchmark_positions(filename, num_samples, num_points=10):\n",
    "    benchmark_tles = read_tle_from_txt(filename, num_samples)\n",
    "    if not benchmark_tles:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    positions = []\n",
    "\n",
    "    for name, tle1, tle2 in benchmark_tles:\n",
    "        satellite = Satrec.twoline2rv(tle1, tle2)\n",
    "        start_time = datetime.now(timezone.utc)\n",
    "        sequence = []\n",
    "\n",
    "        for i in range(num_points):\n",
    "            date_time = start_time + timedelta(seconds=i * 10)\n",
    "            position = get_position_velocity(satellite, date_time)\n",
    "            if position is not None and np.all(np.isfinite(position)):\n",
    "                sequence.append(position)\n",
    "\n",
    "        # Only accept complete valid sequences\n",
    "        if len(sequence) == num_points:\n",
    "            positions.append(sequence)\n",
    "\n",
    "    if not positions:\n",
    "        print(\"No valid satellite sequences generated.\")\n",
    "        return np.array([]), np.array([]) \n",
    "\n",
    "    print(f\"Successfully generated {len(positions)} valid sequences out of {len(benchmark_tles)} requested samples.\")\n",
    "    print(f\"Skipped {len(benchmark_tles) - len(positions)} satellites due to incomplete or invalid sequences.\")\n",
    "\n",
    "    positions = np.array(positions)\n",
    "    return np.array(positions)\n",
    "\n",
    "def generate_advanced_mock_orbits(\n",
    "    num_samples=10, num_points=20, \n",
    "    earth_radius_km=6371, \n",
    "    min_altitude_km=400, max_altitude_km=2000,\n",
    "    eccentricity_range=(0.0, 0.2),\n",
    "    perturb_std_km=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate advanced mock satellite orbits (circular/elliptical, random orientation, with perturbations).\n",
    "    Returns: np.ndarray of shape (num_samples, num_points, 6) -- [x, y, z, vx, vy, vz]\n",
    "    \"\"\"\n",
    "    orbits = []\n",
    "    for _ in range(num_samples):\n",
    "        # Random orbital parameters\n",
    "        a = earth_radius_km + np.random.uniform(min_altitude_km, max_altitude_km)  # semi-major axis\n",
    "        e = np.random.uniform(*eccentricity_range)  # eccentricity\n",
    "        inclination = np.deg2rad(np.random.uniform(0, 180))\n",
    "        raan = np.deg2rad(np.random.uniform(0, 360))\n",
    "        arg_perigee = np.deg2rad(np.random.uniform(0, 360))\n",
    "        # True anomaly points\n",
    "        points = []\n",
    "        for f in np.linspace(0, 2 * np.pi, num_points, endpoint=False):\n",
    "            # Radius at true anomaly f\n",
    "            r = a * (1 - e**2) / (1 + e * np.cos(f))\n",
    "            # Position in orbital plane\n",
    "            x_orb = r * np.cos(f)\n",
    "            y_orb = r * np.sin(f)\n",
    "            z_orb = 0\n",
    "            # Rotation matrix for orientation\n",
    "            # 1. Rotate by argument of perigee\n",
    "            x1 = x_orb * np.cos(arg_perigee) - y_orb * np.sin(arg_perigee)\n",
    "            y1 = x_orb * np.sin(arg_perigee) + y_orb * np.cos(arg_perigee)\n",
    "            z1 = z_orb\n",
    "            # 2. Rotate by inclination\n",
    "            x2 = x1\n",
    "            y2 = y1 * np.cos(inclination) - z1 * np.sin(inclination)\n",
    "            z2 = y1 * np.sin(inclination) + z1 * np.cos(inclination)\n",
    "            # 3. Rotate by RAAN\n",
    "            x = x2 * np.cos(raan) - y2 * np.sin(raan)\n",
    "            y = x2 * np.sin(raan) + y2 * np.cos(raan)\n",
    "            z = z2\n",
    "            # Add small random perturbation for realism\n",
    "            x += np.random.normal(0, perturb_std_km)\n",
    "            y += np.random.normal(0, perturb_std_km)\n",
    "            z += np.random.normal(0, perturb_std_km)\n",
    "            points.append([x, y, z])\n",
    "        points = np.array(points)\n",
    "        velocity = np.gradient(points, axis=0)  # shape (num_points, 3)\n",
    "        seq_with_vel = np.concatenate([points, velocity], axis=1)  # shape (num_points, 6)\n",
    "        orbits.append(seq_with_vel)\n",
    "    return np.array(orbits)  # shape: (num_samples, num_points, 6)\n",
    "\n",
    "def get_future_positions(satellite, start_time, horizons_sec=[600, 1800, 3600]):\n",
    "    \"\"\"\n",
    "    Returns a dict of {horizon_seconds: position (x, y, z)} for each horizon.\n",
    "    \"\"\"\n",
    "    future_positions = {}\n",
    "    for sec in horizons_sec:\n",
    "        dt = start_time + timedelta(seconds=sec)\n",
    "        pos = get_position_velocity(satellite, dt)\n",
    "        if pos is not None and np.all(np.isfinite(pos)):\n",
    "            future_positions[sec] = pos\n",
    "        else:\n",
    "            future_positions[sec] = None\n",
    "    return future_positions\n",
    "\n",
    "def generate_benchmark_positions(filename, num_samples, num_points=20):\n",
    "    benchmark_tles = read_tle_from_txt(filename, num_samples)\n",
    "    if not benchmark_tles:\n",
    "        return np.array([])\n",
    "\n",
    "    positions = []\n",
    "\n",
    "    for name, tle1, tle2 in benchmark_tles:\n",
    "        satellite = Satrec.twoline2rv(tle1, tle2)\n",
    "        start_time = datetime.now(timezone.utc)\n",
    "        sequence = []\n",
    "\n",
    "        for i in range(num_points):\n",
    "            date_time = start_time + timedelta(seconds=i * 10)\n",
    "            position = get_position_velocity(satellite, date_time)\n",
    "            if position is not None and np.all(np.isfinite(position)):\n",
    "                sequence.append(position)\n",
    "\n",
    "        if len(sequence) == num_points:\n",
    "            sequence = np.array(sequence)\n",
    "            velocity = np.gradient(sequence, axis=0)  # shape (num_points, 3)\n",
    "            seq_with_vel = np.concatenate([sequence, velocity], axis=1)  # shape (num_points, 6)\n",
    "            positions.append(seq_with_vel)\n",
    "\n",
    "    if not positions:\n",
    "        print(\"No valid satellite sequences generated.\")\n",
    "        return np.array([])\n",
    "\n",
    "    print(f\"Successfully generated {len(positions)} valid sequences out of {len(benchmark_tles)} requested samples.\")\n",
    "    return np.array(positions)  # shape: (N, num_points, 6)\n",
    "\n",
    "    print(\"X min/max:\", X.min(), X.max())\n",
    "    print(\"y min/max:\", y.min(), y.max())\n",
    "\n",
    "# ================================\n",
    "# Train the LSTM Model\n",
    "# ================================\n",
    "def train_lstm_model(model, positions, epochs, batch_size):\n",
    "    if positions is None or len(positions) == 0 or np.isnan(positions).any():\n",
    "        print(\"Invalid or empty data for LSTM training.\")\n",
    "        return None, None\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    num_sequences, num_points, num_features = positions.shape\n",
    "\n",
    "    positions_reshaped = positions.reshape(-1, num_features)\n",
    "    positions_scaled = scaler.fit_transform(positions_reshaped).reshape(num_sequences, num_points, num_features)\n",
    "\n",
    "    X = positions_scaled[:, :-1, :]  # shape (N, T-1, 6)\n",
    "    y = positions_scaled[:, -1, :3] - positions_scaled[:, -2, :3]  # shape (N, 3)\n",
    "\n",
    "    if X.shape[0] == 0:\n",
    "        print(\"Not enough data for LSTM training.\")\n",
    "        return None, scaler\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X, y,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stop, lr_scheduler]\n",
    "    )\n",
    "\n",
    "    model.save(\"lstm_full_model.keras\")\n",
    "    joblib.dump(scaler, \"lstm_scaler.pkl\")\n",
    "    print(\"Model weights and scaler saved.\")\n",
    "\n",
    "    return model, scaler\n",
    "\n",
    "# ================================\n",
    "# Train the GRU Model\n",
    "# ================================\n",
    "def train_gru_model(model, positions, epochs, batch_size):\n",
    "    # Input validation\n",
    "    if positions is None or len(positions) == 0 or np.isnan(positions).any():\n",
    "        print(\"Invalid or empty data for GRU training.\")\n",
    "        return None, None\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    num_sequences, num_points, num_features = positions.shape\n",
    "\n",
    "    positions_reshaped = positions.reshape(-1, num_features)\n",
    "    positions_scaled = scaler.fit_transform(positions_reshaped).reshape(num_sequences, num_points, num_features)\n",
    "\n",
    "    X = positions_scaled[:, :-1, :]\n",
    "    y = positions_scaled[:, -1, :3]\n",
    "\n",
    "    if X.shape[0] == 0:\n",
    "        print(\"Not enough data for GRU training.\")\n",
    "        return None, scaler\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(\n",
    "        monitor='loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=30,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_scheduler, early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(\"gru_full_model.keras\")\n",
    "    print(\"GRU model weights saved.\")\n",
    "\n",
    "    joblib.dump(scaler, \"gru_scaler.pkl\")\n",
    "\n",
    "    return model, scaler \n",
    "\n",
    "# ================================\n",
    "# Train XGBoost Model on Flattened Time-Series\n",
    "# ================================\n",
    "def train_xgboost_model(positions, epochs=None, batch_size=None, model_path=\"xgboost_model.json\"):\n",
    "    num_sequences, num_timesteps, num_features = positions[:, :-1, :].shape\n",
    "    X = positions[:, :-1, :].reshape(num_sequences, num_timesteps * num_features)\n",
    "    y = positions[:, -1, :3]\n",
    "\n",
    "    # If model exists, load and return it\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"✅ Loaded existing XGBoost model from {model_path}\")\n",
    "        return joblib.load(model_path)\n",
    "\n",
    "    # Else, train a new one\n",
    "    model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        verbosity=1\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✅ Trained and saved XGBoost model to {model_path}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_multi_horizon(model, scaler, input_sequence, horizons_sec=[600,1800,3600]):\n",
    "    \"\"\"\n",
    "    input_sequence: shape (num_points, 3)\n",
    "    Returns: dict {horizon_sec: predicted_position}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    seq = np.copy(input_sequence)\n",
    "    for horizon in horizons_sec:\n",
    "        # Predict next position (roll forward)\n",
    "        X = scaler.transform(seq[-9:]).reshape(1, 9, 3)\n",
    "        pred = model.predict(X)\n",
    "        pred = scaler.inverse_transform(pred).flatten()\n",
    "        # For next step, append prediction to sequence\n",
    "        seq = np.vstack([seq, pred])\n",
    "        results[horizon] = pred\n",
    "    return results\n",
    "\n",
    "# ================================\n",
    "# Load existing weights\n",
    "# ================================\n",
    "def load_existing_model(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loaded full model from {model_path}\")\n",
    "        return keras.models.load_model(model_path)\n",
    "    else:\n",
    "        print(f\"No saved model found at {model_path}. Starting fresh.\")\n",
    "        return None\n",
    "\n",
    "# Converting ECEF to lat/lon\n",
    "def ecef_to_latlon(x, y, z):\n",
    "    lon = np.arctan2(y, x) * (180 / np.pi)\n",
    "    hyp = np.sqrt(x**2 + y**2)\n",
    "    lat = np.arctan2(z, hyp) * (180 / np.pi)\n",
    "    return lat, lon\n",
    "\n",
    "# Function making 3d interactive plot of earth \n",
    "def plot_results(benchmark_positions, predictions, model_name=\"LSTM\"):\n",
    "    benchmark_latlon = np.array([ecef_to_latlon(x, y, z) for x, y, z in benchmark_positions[:, -1, :3]])\n",
    "    predicted_latlon = np.array([ecef_to_latlon(x, y, z) for x, y, z in predictions])\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Adding Earth's surface using a globe texture\n",
    "    lats = np.linspace(-90, 90, 180)\n",
    "    lons = np.linspace(-180, 180, 360)\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    earth_radius = 6371  # Earth radius in km\n",
    "\n",
    "    # Converting lat/lon to ECEF\n",
    "    x = earth_radius * np.cos(np.radians(lat_grid)) * np.cos(np.radians(lon_grid))\n",
    "    y = earth_radius * np.cos(np.radians(lat_grid)) * np.sin(np.radians(lon_grid))\n",
    "    z = earth_radius * np.sin(np.radians(lat_grid))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Surface(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            colorscale='earth',\n",
    "            opacity=0.7 \n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plotting benchmark positions\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=benchmark_positions[:, -1, 0],\n",
    "            y=benchmark_positions[:, -1, 1],\n",
    "            z=benchmark_positions[:, -1, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, color='blue'),\n",
    "            name='Benchmark'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plotting predicted positions\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=predictions[:, 0],\n",
    "            y=predictions[:, 1],\n",
    "            z=predictions[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=4, color='orange'),\n",
    "            name='Prediction'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Layout adjustments\n",
    "    fig.update_layout(\n",
    "        title=f\"{model_name} Satellite Tracking Prediction\",\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "            aspectmode='auto'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=30)\n",
    "    )\n",
    "\n",
    "    fig.show() # Displaying plot\n",
    "\n",
    "# Function to save results of each real position (lat & lon) of a satellite, the LSTM's prediction \n",
    "# of the satellite's real position (lat & lon), and the error in that prediction measured by MSE\n",
    "def save_predictions_for_cesium_with_actual(predictions, benchmark_positions, model_name=\"LSTM\"):\n",
    "    filename = f\"predictions.json\"\n",
    "    output = []\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        orbit_sequence = [\n",
    "            {\"x\": float(x)*1000, \"y\": float(y)*1000, \"z\": float(z)*1000}\n",
    "            for x, y, z in benchmark_positions[idx][:, :3]\n",
    "        ]\n",
    "\n",
    "        output.append({\n",
    "            \"id\": f\"Satellite {idx+1} - {model_name}\",\n",
    "            \"orbit\": orbit_sequence,\n",
    "            \"predicted\": {\n",
    "                \"x\": float(predictions[idx][0])*1000,\n",
    "                \"y\": float(predictions[idx][1])*1000,\n",
    "                \"z\": float(predictions[idx][2])*1000\n",
    "            }\n",
    "        })\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(output, f, indent=4)\n",
    "    print(f\"Saved orbit + prediction data to {filename}\")\n",
    "\n",
    "def save_results_to_csv(benchmark_positions, predictions, model_name=\"LSTM\"):\n",
    "    filename = f\"trackingresults_{model_name}.csv\"\n",
    "    data = []\n",
    "    for i in range(len(predictions)):\n",
    "        actual = benchmark_positions[i][-1][:3]  # <-- FIX: only x, y, z\n",
    "        predicted = predictions[i][:3]           # <-- FIX: only x, y, z\n",
    "\n",
    "        if isinstance(predicted, (list, np.ndarray)) and len(predicted) == 1 and hasattr(predicted[0], \"__len__\"):\n",
    "            predicted = predicted[0]\n",
    "\n",
    "        error = np.linalg.norm(np.array(actual) - np.array(predicted))\n",
    "\n",
    "        data.append({\n",
    "            \"Satellite\": f\"Satellite {i+1}\",\n",
    "            \"Benchmark Latitude\": actual[0],\n",
    "            \"Benchmark Longitude\": actual[1],\n",
    "            \"Benchmark Altitude\": actual[2],\n",
    "            \"Predicted Latitude\": predicted[0],\n",
    "            \"Predicted Longitude\": predicted[1],\n",
    "            \"Predicted Altitude\": predicted[2],\n",
    "            \"Error\": error\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(data).to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "def save_multi_horizon_results_to_csv(all_errors, model_name=\"LSTM\"):\n",
    "    \"\"\"\n",
    "    all_errors: list of dicts with keys: Satellite, Horizon_sec, Prediction, Truth, Error\n",
    "    \"\"\"\n",
    "    filename = f\"multi_horizon_results_{model_name}.csv\"\n",
    "    rows = []\n",
    "    for entry in all_errors:\n",
    "        pred = entry['Prediction']\n",
    "        truth = entry['Truth']\n",
    "        rows.append({\n",
    "            \"Satellite\": entry['Satellite'],\n",
    "            \"Horizon_sec\": entry['Horizon_sec'],\n",
    "            \"Prediction_X\": pred[0],\n",
    "            \"Prediction_Y\": pred[1],\n",
    "            \"Prediction_Z\": pred[2],\n",
    "            \"Truth_X\": truth[0],\n",
    "            \"Truth_Y\": truth[1],\n",
    "            \"Truth_Z\": truth[2],\n",
    "            \"Error\": entry['Error']\n",
    "        })\n",
    "    pd.DataFrame(rows).to_csv(filename, index=False)\n",
    "    print(f\"Multi-horizon results saved to {filename}\")\n",
    "\n",
    "def save_multi_horizon_results_to_json(all_errors, model_name=\"LSTM\"):\n",
    "    filename = f\"multi_horizon_results_{model_name}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(all_errors, f, indent=4)\n",
    "    print(f\"Multi-horizon results saved to {filename}\")\n",
    "\n",
    "def display_error_chart(model_name=\"LSTM\"):\n",
    "    filename = f\"trackingresults_{model_name}.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        df['Error'] = np.sqrt(\n",
    "            (df['Benchmark Latitude'] - df['Predicted Latitude'])**2 +\n",
    "            (df['Benchmark Longitude'] - df['Predicted Longitude'])**2 +\n",
    "            (df['Benchmark Altitude'] - df['Predicted Altitude'])**2\n",
    "        )\n",
    "        df['Distance Missed (km)'] = df['Error']\n",
    "\n",
    "        df['Vector Magnitude'] = np.sqrt(\n",
    "            df['Benchmark Latitude']**2 +\n",
    "            df['Benchmark Longitude']**2 +\n",
    "            df['Benchmark Altitude']**2\n",
    "        )\n",
    "        df = df[df['Vector Magnitude'] > 1000]\n",
    "\n",
    "        df['Error Percentage'] = (df['Error'] / df['Vector Magnitude']) * 100\n",
    "\n",
    "        # Create side-by-side subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        # Plot 1: Error Percentage\n",
    "        axes[0].plot(df.index, df['Error Percentage'], label='Error Percentage (%)', color='blue', marker='o')\n",
    "        axes[0].set_title(f'{model_name} Prediction Error Percentage')\n",
    "        axes[0].set_xlabel('Prediction Number')\n",
    "        axes[0].set_ylabel('Error Percentage (%)')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid()\n",
    "\n",
    "        # Plot 2: Distance Missed\n",
    "        axes[1].plot(df.index, df['Distance Missed (km)'], label='Distance Missed (km)', color='red', marker='x')\n",
    "        axes[1].set_title(f'{model_name} Prediction Distance Missed')\n",
    "        axes[1].set_xlabel('Prediction Number')\n",
    "        axes[1].set_ylabel('Distance Missed (km)')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating chart: {e}\")\n",
    "\n",
    "\n",
    "# Place this after all your function definitions, but before the GUI code.\n",
    "# For example, right before:\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_gui()\n",
    "\n",
    "# --- BEGIN DEBUG CELL ---\n",
    "\n",
    "# Generate mock benchmark_positions for debugging\n",
    "benchmark_positions = generate_advanced_mock_orbits(num_samples=5, num_points=20)\n",
    "\n",
    "positions = benchmark_positions[:5]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "positions_reshaped = positions.reshape(-1, 6)  # Use all 6 features!\n",
    "positions_scaled = scaler.fit_transform(positions_reshaped).reshape(5, positions.shape[1], 6)\n",
    "\n",
    "X = positions_scaled[:, :-1, :]  # shape (N, T-1, 6)\n",
    "y = positions_scaled[:, -1, :3] - positions_scaled[:, -2, :3]  # shape (N, 3)\n",
    "\n",
    "model = build_lstm_model()\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "\n",
    "predictions_scaled = model.predict(X)\n",
    "# Pad predictions with zeros for velocity columns before inverse_transform\n",
    "predictions_padded = np.hstack([predictions_scaled, np.zeros((predictions_scaled.shape[0], 3))])\n",
    "predictions = scaler.inverse_transform(predictions_padded)[:, :3]\n",
    "\n",
    "print(\"True:\", positions[:, -1, :3])\n",
    "print(\"Pred:\", predictions)\n",
    "print(\"Error:\", np.linalg.norm(positions[:, -1, :3] - predictions, axis=1))\n",
    "\n",
    "# --- END DEBUG CELL ---\n",
    "\n",
    "# ================================\n",
    "# GUI Setup \n",
    "# ================================\n",
    "def run_gui():\n",
    "    def start_prediction(event=None):\n",
    "        filename = filedialog.askopenfilename(title=\"Select TLE File\")\n",
    "        benchmark_positions = None\n",
    "        selected_model = model_var.get() \n",
    "\n",
    "        if not filename:\n",
    "            # Ask user if they want to use mock orbits\n",
    "            use_mock = messagebox.askyesno(\"No file selected\", \"No TLE file selected. Use advanced mock orbits?\")\n",
    "            if use_mock:\n",
    "                num_samples = int(sample_entry.get())\n",
    "                benchmark_positions = generate_advanced_mock_orbits(num_samples=num_samples, num_points=10)\n",
    "                print(f\"Generated {num_samples} advanced mock orbits.\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Error\", \"No file selected\")\n",
    "                return\n",
    "        else:\n",
    "            try:\n",
    "                num_samples = int(sample_entry.get())\n",
    "                epochs = int(epoch_entry.get())\n",
    "                batch_size = int(batch_entry.get())\n",
    "\n",
    "                if \"RK4\" in filename.upper():\n",
    "                    rk4_data = read_simulated_orbit_file(filename)\n",
    "                    if rk4_data.shape[0] < 10:\n",
    "                        messagebox.showerror(\"Error\", \"RK4 orbit data too short for prediction.\")\n",
    "                        return\n",
    "                    benchmark_positions = rk4_data  # already shape: (N, 10, 3)\n",
    "                    print(f\"Loaded {benchmark_positions.shape[0]} simulated sequences.\")\n",
    "                else:\n",
    "                    benchmark_positions = generate_benchmark_positions(filename, num_samples, num_points=20)\n",
    "\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", str(e))\n",
    "                return\n",
    "            \n",
    "        print(\"Any NaNs in benchmark_positions?\", np.isnan(benchmark_positions).any())\n",
    "        print(\"Min/max x:\", np.min(benchmark_positions[...,0]), np.max(benchmark_positions[...,0]))\n",
    "        print(\"Min/max y:\", np.min(benchmark_positions[...,1]), np.max(benchmark_positions[...,1]))\n",
    "        print(\"Min/max z:\", np.min(benchmark_positions[...,2]), np.max(benchmark_positions[...,2]))\n",
    "\n",
    "        try:\n",
    "            num_samples = int(sample_entry.get().strip().replace('`', ''))\n",
    "            epochs = int(epoch_entry.get().strip().replace('`', ''))\n",
    "            batch_size = int(batch_entry.get().strip().replace('`', ''))\n",
    "\n",
    "            if selected_model == \"LSTM\":\n",
    "                model_path = \"lstm_full_model.keras\"\n",
    "                scaler_path = \"lstm_scaler.pkl\"  # Make sure this matches training code\n",
    "\n",
    "                if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
    "                    model = keras.models.load_model(model_path)\n",
    "                    scaler = joblib.load(scaler_path)\n",
    "                    print(f\"Loaded LSTM model and scaler.\")\n",
    "                else:\n",
    "                    model = build_lstm_model()\n",
    "                    print(\"No saved LSTM model found. Creating new one.\")\n",
    "                    model, scaler = train_lstm_model(model, benchmark_positions, epochs, batch_size)\n",
    "\n",
    "                # --- FIXED SCALING FOR PREDICTION ---\n",
    "                num_samples = benchmark_positions.shape[0]\n",
    "                num_timesteps = benchmark_positions.shape[1] - 1\n",
    "\n",
    "                X = benchmark_positions[:, :-1, :]  # shape (N, T-1, 6)\n",
    "                X_scaled = scaler.transform(X.reshape(-1, 6)).reshape(num_samples, num_timesteps, 6)\n",
    "                delta_pred = model.predict(X_scaled)  # shape (N, 3)\n",
    "                last_pos_scaled = X_scaled[:, -1, :3]  # shape (N, 3)\n",
    "                pred_scaled = last_pos_scaled + delta_pred  # predicted next position in scaled space\n",
    "\n",
    "                last_vels = X_scaled[:, -1, 3:6]\n",
    "                pred_padded = np.hstack([pred_scaled, last_vels])\n",
    "                predictions = scaler.inverse_transform(pred_padded)[:, :3]\n",
    "\n",
    "                print(\"Sample true:\", benchmark_positions[:5, -1, :3])\n",
    "                print(\"Sample pred:\", predictions[:5])\n",
    "                print(\"Sample error:\", np.linalg.norm(benchmark_positions[:5, -1, :3] - predictions[:5], axis=1))\n",
    "\n",
    "            elif selected_model == \"GRU\":\n",
    "                model_path = \"gru_full_model.keras\"\n",
    "                scaler_path = \"gru_scaler.pkl\"\n",
    "\n",
    "                if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
    "                    model = keras.models.load_model(model_path)\n",
    "                    scaler = joblib.load(scaler_path)\n",
    "                    print(f\"Loaded GRU model and scaler.\")\n",
    "                else:\n",
    "                    model = build_gru_model()\n",
    "                    print(\"No saved GRU model found. Creating new one.\")\n",
    "                    model, scaler = train_gru_model(model, benchmark_positions, epochs, batch_size)\n",
    "\n",
    "                # --- FIXED SCALING FOR PREDICTION ---\n",
    "                # --- DELTA LOGIC FOR PREDICTION ---\n",
    "                num_samples = benchmark_positions.shape[0]\n",
    "                num_timesteps = benchmark_positions.shape[1] - 1\n",
    "                X = benchmark_positions[:, :-1, :]  # shape (N, 19, 6)\n",
    "                X_scaled = scaler.transform(X.reshape(-1, 6)).reshape(num_samples, num_timesteps, 6)\n",
    "                delta_pred = model.predict(X_scaled)  # shape (N, 3)\n",
    "                last_pos_scaled = X_scaled[:, -1, :3]  # shape (N, 3)\n",
    "                pred_scaled = last_pos_scaled + delta_pred  # predicted next position in scaled space\n",
    "\n",
    "                last_vels = X_scaled[:, -1, 3:6]\n",
    "                pred_padded = np.hstack([pred_scaled, last_vels])\n",
    "                predictions = scaler.inverse_transform(pred_padded)[:, :3]\n",
    "\n",
    "                print(\"Sample true:\", benchmark_positions[:5, -1, :3])\n",
    "                print(\"Sample pred:\", predictions[:5])\n",
    "                print(\"Sample error:\", np.linalg.norm(benchmark_positions[:5, -1, :3] - predictions[:5], axis=1))\n",
    "\n",
    "            elif selected_model == \"XGBoost\":\n",
    "                model_path = \"xgboost_model.json\"\n",
    "                num_samples, num_timesteps, num_features = benchmark_positions[:, :-1, :].shape\n",
    "                X_flattened = benchmark_positions[:, :-1, :].reshape(num_samples, num_timesteps * num_features)\n",
    "                y = benchmark_positions[:, -1, :3]            \n",
    "        \n",
    "                # Try to load model, else retrain\n",
    "                retrain = False\n",
    "                if os.path.exists(model_path):\n",
    "                    model = joblib.load(model_path)\n",
    "                    # Check if feature shape matches\n",
    "                    if hasattr(model, 'n_features_in_') and model.n_features_in_ != X_flattened.shape[1]:\n",
    "                        print(f\"XGBoost model feature mismatch: expected {model.n_features_in_}, got {X_flattened.shape[1]}. Retraining.\")\n",
    "                        retrain = True\n",
    "                else:\n",
    "                    retrain = True\n",
    "\n",
    "                if retrain:\n",
    "                    model = xgb.XGBRegressor(\n",
    "                        objective='reg:squarederror',\n",
    "                        n_estimators=100,\n",
    "                        learning_rate=0.1,\n",
    "                        verbosity=1\n",
    "                    )\n",
    "                    model.fit(X_flattened, y)\n",
    "                    joblib.dump(model, model_path)\n",
    "                    print(f\"✅ Trained and saved XGBoost model to {model_path}\")\n",
    "\n",
    "                predictions = model.predict(X_flattened)\n",
    "\n",
    "            print(\"Predictions complete.\")\n",
    "\n",
    "            errors = np.linalg.norm(benchmark_positions[:, -1, :3] - predictions[:, :3], axis=1)\n",
    "            outlier_indices = np.where(errors > 10000)[0]  # You can adjust this threshold as needed\n",
    "            if len(outlier_indices) > 0:\n",
    "                print(\"Outlier indices (error > 10,000 km):\", outlier_indices)\n",
    "                for idx in outlier_indices:\n",
    "                    print(f\"Satellite {idx+1}:\")\n",
    "                    print(f\"  True: {benchmark_positions[idx, -1, :3]}\") \n",
    "                    print(f\"  Pred: {predictions[idx, :3]}\")\n",
    "                    print(f\"  Error: {errors[idx]:.2f} km\")\n",
    "            else:\n",
    "                print(\"No large outliers detected (all errors <= 10,000 km).\")\n",
    "\n",
    "            save_predictions_for_cesium_with_actual(predictions, benchmark_positions, model_name=selected_model)\n",
    "            save_results_to_csv(benchmark_positions, predictions, model_name=selected_model)\n",
    "            plot_results(benchmark_positions, predictions, model_name=selected_model)\n",
    "            display_error_chart(model_name=selected_model)\n",
    "\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Satellite Tracker\")\n",
    "\n",
    "    # Input for number of samples\n",
    "    tk.Label(root, text=\"Number of Satellites:\").grid(row=0, column=0)\n",
    "    sample_entry = tk.Entry(root)\n",
    "    sample_entry.grid(row=0, column=1)\n",
    "\n",
    "    # Input for epochs\n",
    "    tk.Label(root, text=\"Epochs:\").grid(row=1, column=0)\n",
    "    epoch_entry = tk.Entry(root)\n",
    "    epoch_entry.grid(row=1, column=1)\n",
    "\n",
    "    # Input for batch size\n",
    "    tk.Label(root, text=\"Batch Size:\").grid(row=2, column=0)\n",
    "    batch_entry = tk.Entry(root)\n",
    "    batch_entry.grid(row=2, column=1)\n",
    "\n",
    "    # Dropdown for selecting model\n",
    "    tk.Label(root, text=\"Select Model:\").grid(row=3, column=0)\n",
    "    model_var = StringVar(root)\n",
    "    model_var.set(\"LSTM\")  # Default model\n",
    "    model_dropdown = OptionMenu(root, model_var, \"LSTM\", \"GRU\", \"XGBoost\")\n",
    "    model_dropdown.grid(row=3, column=1)\n",
    "\n",
    "    # Start Prediction button\n",
    "    tk.Button(root, text=\"Start Prediction\", command=start_prediction).grid(row=4, column=0, columnspan=2)\n",
    "\n",
    "    # Trigger start with \"Enter\" key\n",
    "    root.bind('<Return>', start_prediction)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "# --- BEGIN DEBUG CELL ---\n",
    "\n",
    "print(\"=== DEBUG: SCALER & MODEL PIPELINE ===\")\n",
    "\n",
    "# Generate a tiny batch of mock orbits\n",
    "mock_positions = generate_advanced_mock_orbits(num_samples=3, num_points=20)\n",
    "print(\"Mock positions shape:\", mock_positions.shape)\n",
    "\n",
    "# Fit scaler on all data (simulate training)\n",
    "scaler = MinMaxScaler()\n",
    "mock_reshaped = mock_positions.reshape(-1, 6)\n",
    "mock_scaled = scaler.fit_transform(mock_reshaped).reshape(mock_positions.shape)\n",
    "\n",
    "# Prepare X and y\n",
    "X = mock_scaled[:, :-1, :]  # (3, 19, 6)\n",
    "y = mock_scaled[:, -1, :3]  # (3, 3)\n",
    "\n",
    "# Build and train a tiny model\n",
    "model = build_lstm_model()\n",
    "model.fit(X, y, epochs=10, batch_size=1, verbose=2)\n",
    "\n",
    "# Predict\n",
    "pred_scaled = model.predict(X)\n",
    "last_vels = mock_positions[:, -1, 3:6]\n",
    "pred_padded = np.hstack([pred_scaled, last_vels])\n",
    "pred_unscaled = scaler.inverse_transform(pred_padded)[:, :3]\n",
    "\n",
    "print(\"True last positions:\\n\", mock_positions[:, -1, :3])\n",
    "print(\"Predicted last positions:\\n\", pred_unscaled)\n",
    "print(\"Error (km):\", np.linalg.norm(mock_positions[:, -1, :3] - pred_unscaled, axis=1))\n",
    "\n",
    "# --- END DEBUG CELL ---\n",
    "\n",
    "# ================================\n",
    "# Run the GUI\n",
    "# ================================\n",
    "if __name__ == \"__main__\":  \n",
    "    run_gui()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
